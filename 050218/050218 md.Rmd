---
title: "050218"
author: "CC"
date: "May 2, 2018"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Unsupervised learning analysis of cancer cells

input data

```{r}

url <- "https://bioboot.github.io/bggn213_S18/class-material/WisconsinCancer.csv"


wisc.df <- read.csv(url)

head(wisc.df)
```

How may dianosis are cancer vs no cancer

```{r}

table(wisc.df$diagnosis)

```

Convert excluding the first two column with non-numeric values to a matrix

```{r}

#Convert the features of the data: wisc.data
##also remove the column 33 because it has the NAs
wisc.data <- as.matrix (wisc.df [-c(1, 2, 33)])


#selecting now the id column as the row names instead of the numerical ordered values
row.names(wisc.data) <- wisc.df$id


```


```{r}
head(wisc.data)
```

#create diagnosis vector by completing the mission code

```{r}

diagnosis <- as.numeric (wisc.df$diagnosis == "M")


#knowing how many people have the malignant diagnosis
sum(diagnosis)
```

#know how many patients do we have
```{r}
nrow(wisc.data)


#can do dim() and it will give us both dimensions of the data

```



How many variables in the data are suffixed with the _mean
```{r}
#first is the prefix, second where to look for in your data, value is so you can see the names of the actual variable instead of getting a numeric value

length(grep("mean", colnames(wisc.data), value = TRUE))

#length will give you the total values of the variables that have the preffix



```


Principal component analysis (PCA)

```{r}


plot(colMeans(wisc.data), type = "o")
```



Perform PCA on wis.data

```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
```


How are we doing
```{r}
summary(wisc.pr)
```


##Plot PCA RESULTS

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis+1)
```


Scree-plot variance explained

```{r}
pve <- wisc.pr$sdev^2 / sum(wisc.pr$sdev^2)
# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```


```{r}
barplot(pve, names.arg = paste ("PC", 1:length(pve)), las = 2, axes = FALSE, ylab = "Proportion of varience")

#we want each bar to have the name of pve
#we want the labels to be rounded to two digitals
axis(2, at = pve, labels = round(pve,2)*100)
```

#Section 3
Hierarchical clustering of case data

##Scale the wisc.dat data: data.scaled
```{r}
data.scaled <- scale(wisc.data)
```


##Calculate the Euclidean distances between all pairs 

```{r}
data.dist <- dist(data.scaled)
```


##Create a hierarchical clustering model using complete linkage

```{r}
wisc.hclust <- hclust(data.dist, method ="complete")
```

Lets plot our tree

```{r}
plot(wisc.hclust)
abline(h=20, col="red", lwd=3)
```

Lets cut the tree
#Selecting number of clusters

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)

```


##How do these groups match our 'diagnosis'
```{r}
table(wisc.hclust.clusters, diagnosis)
```



Section 4
#Creating k-means model o n wisc.data

```{r}
wisc.km <- kmeans(data.scaled, centers=2, nstart = 20)
table(wisc.km$cluster)
```

##compare to expert 'diagnosis'
```{r}
table(wisc.km$cluster, diagnosis)
```





Section 5
##Clustering on PCA results

```{r}
wisc.pr.hclust <- hclust (dist(wisc.pr$x[,1:3]), method = "ward.D2")
plot(wisc.pr.hclust)

```



#Cut this hierarchical clustering

```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=4)

plot(wisc.pr$x[,1:2], col=wisc.pr.hclust.clusters)
```

Compare to actual diagnoses
```{r}
table(wisc.pr.hclust.clusters, diagnosis)
```








```{r}
#install.packages("rgl")
```


```{r}
#library(rgl)
#plot3d(wisc.pr$x)
```






